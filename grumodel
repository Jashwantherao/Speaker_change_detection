import torch
import torch.nn as nn
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

class StepActivation(nn.Module):
    def __init__(self):
        super(StepActivation, self).__init__()
        self.threshold = 0.4

    def forward(self, x):
        cond = x < self.threshold
        out = torch.where(cond, torch.zeros_like(x), torch.ones_like(x))
        return out

class Net(nn.Module):
    def __init__(self, featureplan):
        super(Net, self).__init__()
        self.featureplan = featureplan
        if (featureplan=="mfcc.txt"):
            self.frame_shape = (800, 40)
        elif (featureplan=="pyannote_based.txt"):
            self.frame_shape = (800, 35)
        else:
            print ("Incompatible featureplan")
            raise

        self.lstm1 = nn.LSTM(input_size=self.frame_shape[1], hidden_size=64, bidirectional=True, num_layers=1)
        self.lstm2 = nn.LSTM(input_size=128, hidden_size=32, bidirectional=True, num_layers=1)
        self.dense1 = nn.Linear(in_features=64, out_features=40)
        self.dense2 = nn.Linear(in_features=40, out_features=10)
        self.dense3 = nn.Linear(in_features=10, out_features=1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x, _ = self.lstm1(x)
        x, _ = self.lstm2(x)
        x = self.dense1(x)
        x = torch.tanh(x)
        x = self.dense2(x)
        x = torch.tanh(x)
        x = self.dense3(x)
        x = self.sigmoid(x)
        return x

def create_model(featureplan):
    model = Net(featureplan)
    criterion = nn.BCELoss()
    optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9, eps=1e-4, weight_decay=1e-5)
    return model, criterion, optimizer






model, criterion, optimizer = create_model(featureplan)

# Training loop
for epoch in range(num_epochs):
    for i, (inputs, labels) in enumerate(train_loader):
        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Print loss every epoch
    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))
